import math
from functools import partial
import random
import time
from typing import NamedTuple
import jax
import jax.numpy as jnp
from flax import nnx
from einops import rearrange
import optuna
from rich.progress import track
from rich.console import Console

from mapox import create_env, TimeStep, Environment

from jaxrl.config import Config, PPOConfig
from jaxrl.experiment import Experiment
from jaxrl.optimizer import create_optimizer
from jaxrl.model.network import TransformerActorCritic
from jaxrl.rollout import Rollout, RolloutState
from jaxrl.checkpointer import Checkpointer
from jaxrl.util import count_parameters, format_count, lerp


class TrainingLogs(NamedTuple):
    value_loss: jax.Array
    actor_loss: jax.Array
    entropy_loss: jax.Array
    total_loss: jax.Array
    entropy_coef: jax.Array


def create_training_logs() -> TrainingLogs:
    return TrainingLogs(
        value_loss=jnp.array(0.0),
        actor_loss=jnp.array(0.0),
        entropy_loss=jnp.array(0.0),
        total_loss=jnp.array(0.0),
        entropy_coef=jnp.array(0.0),
    )


def add_seq_dim(ts: TimeStep):
    return jax.tree.map(lambda x: rearrange(x, "b ... -> b 1 ..."), ts)


def evaluate(
    model: TransformerActorCritic,
    rollout: Rollout,
    rngs: nnx.Rngs,
    env: Environment,
    hypers: PPOConfig,
    league_model: TransformerActorCritic | None = None,
    progress: jax.Array | None = None,
):
    reset_key = rngs.env()
    env_state, timestep = env.reset(reset_key)

    rollout_state = rollout.create_state()
    carry = model.initialize_carry(rollout.batch_size, rngs)

    agent_idx = None
    league_size = None
    league_carry = None
    if league_model is not None:
        agent_idx = jax.random.permutation(rngs.env(), env.num_agents)
        agent_inv_idx = jnp.zeros_like(agent_idx).at[agent_idx].set(jnp.arange(env.num_agents))

        league_size = env.num_agents - rollout.batch_size
        league_carry = league_model.initialize_carry(league_size, rngs)

    def split_timestep(ts: TimeStep) -> tuple[TimeStep, TimeStep | None]:
        if agent_idx is None:
            return ts, None

        ts = jax.tree.map(lambda xs: xs[agent_idx], ts)
        league_timestep = jax.tree.map(lambda xs: xs[:league_size], ts)
        timestep = jax.tree.map(lambda xs: xs[league_size:], ts)
        return timestep, league_timestep

    def _step(i, x):
        rollout_state, rngs, env_state, env_timestep, carry, league_carry = x

        action_key = rngs.action()
        env_key = rngs.env()

        if agent_idx is not None:
            timestep, league_timestep = split_timestep(env_timestep)

            _, league_policy, league_carry = league_model(add_seq_dim(league_timestep), league_carry)
            league_actions = league_policy.sample(seed=rngs.action()).squeeze(axis=-1)
        else:
            league_actions = None
            timestep = env_timestep

        value_rep, policy, carry = model(add_seq_dim(timestep), carry)

        action = policy.sample(seed=action_key)
        log_prob = policy.log_prob(action).squeeze(axis=-1)
        action = action.squeeze(axis=-1)
        value = model.get_value(value_rep).squeeze(axis=-1)

        if league_actions is not None:
            action = jnp.concatenate((league_actions, action), axis=0)
            action = action[agent_inv_idx]

        env_state, next_timestep = env.step(env_state, action, env_key)

        rollout_next_timestep, _ = split_timestep(next_timestep)

        rollout_state = rollout.store(
            rollout_state,
            step=i,
            timestep=timestep,
            next_timestep=rollout_next_timestep,
            log_prob=log_prob,
            value=value,
        )

        return rollout_state, rngs, env_state, next_timestep, carry, league_carry

    rollout_state, rngs, env_state, _, _, _ = nnx.fori_loop(
        0,
        rollout.trajectory_length,
        _step,
        init_val=(rollout_state, rngs, env_state, timestep, carry, league_carry),
    )

    # save the last value
    timestep, _ = split_timestep(timestep)
    value_rep, _, _ = model(add_seq_dim(timestep), carry)
    value = model.get_value(value_rep).squeeze(axis=-1)
    rollout_state = rollout_state._replace(values=rollout_state.values.at[:, -1].set(value))

    rollout_state = rollout.calculate_advantage(
        rollout_state, discount=hypers.discount, gae_lambda=hypers.gae_lambda, norm_adv=hypers.normalize_advantage
    )

    env_logs = env.create_logs(env_state)

    return rollout_state, env_logs, rngs


def ppo_loss(
    model: TransformerActorCritic,
    rollout: RolloutState,
    hypers: PPOConfig,
    progress: jax.Array,
):
    batch_obs = rollout.obs
    batch_target = rollout.targets
    batch_log_prob = rollout.log_prob
    batch_actions = rollout.actions
    batch_action_masks = rollout.action_mask
    batch_advantage = rollout.advantages
    batch_terminated = rollout.terminated

    batch_last_actions = rollout.last_actions
    batch_last_rewards = rollout.last_rewards
    batch_task_ids = rollout.task_ids

    positions = jnp.arange(batch_obs.shape[1], dtype=jnp.int32)[None, :]

    value_rep, policy, _ = model(
        TimeStep(
            obs=batch_obs,
            time=positions,
            terminated=batch_terminated,
            last_action=batch_last_actions,
            last_reward=batch_last_rewards,
            action_mask=batch_action_masks,
            task_ids=batch_task_ids,
        ),
    )
    log_probs = policy.log_prob(batch_actions)

    value_loss = model.get_value_loss(value_rep, batch_target)

    ratio = jnp.exp(log_probs - batch_log_prob)

    pg_loss1 = ratio * batch_advantage
    pg_loss2 = (
        jnp.clip(ratio, 1.0 - hypers.vf_clip, 1.0 + hypers.vf_clip) * batch_advantage
    )

    actor_loss = -jnp.minimum(pg_loss1, pg_loss2).mean()

    entropy_loss = -policy.entropy().mean()

    entropy_coef_value = lerp(hypers.entropy_coef, hypers.entropy_coef_end, progress)

    total_loss = (
        hypers.vf_coef * value_loss + actor_loss + entropy_coef_value * entropy_loss
    )

    logs = TrainingLogs(
        value_loss=value_loss,
        actor_loss=actor_loss,
        entropy_loss=entropy_loss,
        total_loss=total_loss,
        entropy_coef=entropy_coef_value,
    )

    return total_loss, logs


def train(
    optimizer: nnx.Optimizer,
    rngs: nnx.Rngs,
    step: jax.Array,
    rollout: Rollout,
    env: Environment,
    config: Config,
    fictitious_model: TransformerActorCritic | None = None
):
    hypers = config.learner.trainer

    @partial(nnx.grad, has_aux=True)
    def _vec_grad(model, rollout_state, progress):
        return ppo_loss(model, rollout_state, hypers, progress)

    def _minibatch_step(carry, rollout_state):
        optimizer, logs, progress = carry
        grad, step_logs = _vec_grad(optimizer.model, rollout_state, progress)

        optimizer.update(grad)
        logs = jax.tree.map(lambda x, y: x + y, logs, step_logs)

        return (optimizer, logs, progress)

    def _epoch_step(i, x):
        optimizer, rollout_state, logs, rngs, progress = x

        minibatch_rng = rngs.shuffle()
        minibatch_rollout_state = rollout.create_minibatches(
            rollout_state, hypers.minibatch_count, minibatch_rng
        )
        optimizer, logs, progress = nnx.scan(
            _minibatch_step, in_axes=(nnx.Carry, 0), out_axes=nnx.Carry
        )((optimizer, logs, progress), minibatch_rollout_state)

        return optimizer, rollout_state, logs, rngs, progress

    def _global_step(i, x):
        optimizer, logs, env_logs, rngs, step_value = x
        progress = step_value / config.update_steps

        rollout_state, env_log_update, rngs = evaluate(
            optimizer.model, rollout, rngs, env, hypers, fictitious_model, progress
        )
        optimizer, rollout_state, logs, rngs, _ = nnx.fori_loop(
            0,
            hypers.epoch_count,
            _epoch_step,
            init_val=(
                optimizer,
                rollout_state,
                logs,
                rngs,
                progress,
            ),
        )

        env_logs = jax.tree.map(lambda x, y: x + y, env_logs, env_log_update)

        return optimizer, logs, env_logs, rngs, step_value + 1

    logs = create_training_logs()
    env_logs = env.create_placeholder_logs()

    optimizer, logs, env_logs, rngs, step = nnx.fori_loop(
        0,
        config.updates_per_jit,
        _global_step,
        init_val=(optimizer, logs, env_logs, rngs, step),
    )

    logs = jax.tree.map(
        lambda x: x
        / (config.updates_per_jit * hypers.epoch_count * hypers.minibatch_count),
        logs,
    )
    env_logs = jax.tree.map(lambda x: x / config.updates_per_jit, env_logs)

    return optimizer, rngs, step, {"algo": logs._asdict(), "env": env_logs}


def replicate_model(optimizer, sharding):
    state = nnx.state(optimizer)
    state = jax.device_put(state, sharding)
    nnx.update(optimizer, state)


def block_all(xs):
    return jax.tree_util.tree_map(lambda x: x.block_until_ready(), xs)


def train_run(
    experiment: Experiment, trial: optuna.Trial | None = None, profile: bool = False
):
    console = Console()

    max_steps = experiment.config.max_env_steps
    has_league = experiment.config.snapshot_league

    logger = experiment.create_logger(console)
    checkpointer = Checkpointer(experiment.checkpoints_url)

    env, task_count = create_env(
        experiment.config.environment, max_steps, experiment.config.num_envs
    )

    batch_size = env.num_agents

    # todo the rollout is smaller because half the agents are the factious copy, these are not used for training
    rollout_size = batch_size
    if has_league:
        rollout_size //= 2

    rngs = nnx.Rngs(default=experiment.default_seed)
    rollout = Rollout(rollout_size, max_steps, env.observation_spec, env.action_spec)

    model = TransformerActorCritic(
        experiment.config.learner.model,
        env.observation_spec,
        env.action_spec.num_actions,
        max_seq_length=max_steps,
        task_count=task_count,
        rngs=rngs,
    )

    optimizer = nnx.ModelAndOptimizer(
        model=model,
        tx=create_optimizer(
            experiment.config.learner.optimizer,
            experiment.config.update_steps
            * experiment.config.learner.trainer.minibatch_count
            * experiment.config.learner.trainer.epoch_count,
        ),
        wrt=nnx.Param,
    )

    rngs = nnx.Rngs(default=jax.random.PRNGKey(experiment.default_seed))

    jitted_train = nnx.jit(train, static_argnums=(3, 4, 5))

    env_steps_per_update = (
        batch_size
        * max_steps
        # * device_rngs.shape[0]
        * experiment.config.updates_per_jit
    )
    outer_updates = experiment.config.update_steps // experiment.config.updates_per_jit

    console.print(f"Starting Training: {experiment.unique_token}")
    console.print(f"Parameter Count: {count_parameters(model)}")
    console.print(f"Agent Count: {env.num_agents}")

    snapshot_league = None
    if has_league:
        snapshot_league = [nnx.clone(model)]

    checkpoint_interval: int | None = None
    if experiment.config.num_checkpoints > 0 and outer_updates > 0:
        checkpoint_interval = max(
            1,
            math.ceil(outer_updates / experiment.config.num_checkpoints),
        )

    logs = None
    step = jnp.asarray(0, dtype=jnp.int32)
    for i in track(range(outer_updates), description="Training", console=console):
        start_time = time.time()

        league_model = None
        if snapshot_league is not None:
            league_model = random.choice(snapshot_league)

        optimizer, rngs, step, logs = jitted_train(
            optimizer, rngs, step, rollout, env, experiment.config, league_model
        )

        if profile and i >= 4:
            with jax.profiler.trace("/tmp/jax-trace"):
                optimizer, rngs, step, logs = jitted_train(
                    optimizer, rngs, step, rollout, env, experiment.config
                )
                block_all(nnx.state(optimizer))

            break

        # this should be delayed n-1 for jax to use async dispatch
        logger.log(logs, i)

        stop_time = time.time()

        delta_time = stop_time - start_time
        console.print(
            f"Steps per second: {format_count(int(env_steps_per_update / delta_time))}"
        )
        console.print(
            f"Updates per second: {experiment.config.updates_per_jit / delta_time}"
        )

        if trial:
            trial.report(logs.rewards.item(), i)

        if (
            checkpoint_interval is not None
            and (i + 1) % checkpoint_interval == 0
        ):
            completed_updates = (i + 1) * experiment.config.updates_per_jit
            checkpointer.save(optimizer.model, completed_updates)

            if snapshot_league is not None:
                snapshot_league.append(nnx.clone(optimizer.model))

    checkpointer.save(optimizer.model, experiment.config.update_steps)

    logger.close()
    checkpointer.close()

    return -1.0
